{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll implement an L-layered deep neural network and train it on the MNIST dataset. The MNIST dataset contains scanned images of handwritten digits, along with their correct classification labels (between 0-9). MNIST's name comes from the fact that it is a modified subset of two data sets collected by NIST, the United States' National Institute of Standards and Technology.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset we use here is 'mnist.pkl.gz' which is divided into training, validation and test data. The following function <i> load_data() </i> unpacks the file and extracts the training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    #We are segregating the data into train ,validation and test data\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    f.seek(0)\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n array([5, 0, 4, ..., 8, 4, 8]))"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(50000, 784)\n(50000,)\n"
    }
   ],
   "source": [
    "# shape of data\n",
    "print(training_data[0].shape)\n",
    "print(training_data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The feature dataset is:[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\nThe target dataset is:[5 0 4 ... 8 4 8]\nThe number of examples in the training dataset is:50000\nThe number of points in a single input is:784\n"
    }
   ],
   "source": [
    "print(\"The feature dataset is:\" + str(training_data[0]))\n",
    "print(\"The target dataset is:\" + str(training_data[1]))\n",
    "print(\"The number of examples in the training dataset is:\" + str(len(training_data[0])))\n",
    "print(\"The number of points in a single input is:\" + str(len(training_data[0][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as discussed earlier in the lectures, the target variable is converted to a one hot matrix. We use the function <i> one_hot </i> to convert the target dataset to one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(j):\n",
    "    # input is the target dataset of shape (m,) where m is the number of data points\n",
    "    # returns a 2 dimensional array of shape (10, m) where each target value is converted to a one hot encoding\n",
    "    # Look at the next block of code for a better understanding of one hot encoding\n",
    "    n = j.shape[0]\n",
    "    new_array = np.zeros((10, n))\n",
    "    index = 0\n",
    "    for res in j:\n",
    "        new_array[res][index] = 1.0\n",
    "        index = index + 1\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(10,)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "data = np.array([0, 1, 2, 6, 4, 5, 6, 7, 8, 9])\n",
    "print(data.shape)\n",
    "one_hot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function data_wrapper() will convert the dataset into the desired shape and also convert the ground truth labels to one_hot matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    \n",
    "    training_inputs = np.array(tr_d[0][:]).T\n",
    "    training_results = np.array(tr_d[1][:])\n",
    "    train_set_y = one_hot(training_results)\n",
    "    \n",
    "    validation_inputs = np.array(va_d[0][:]).T\n",
    "    validation_results = np.array(va_d[1][:])\n",
    "    validation_set_y = one_hot(validation_results)\n",
    "    \n",
    "    test_inputs = np.array(te_d[0][:]).T\n",
    "    test_results = np.array(te_d[1][:])\n",
    "    test_set_y = one_hot(test_results)\n",
    "    \n",
    "    return (training_inputs, train_set_y, test_inputs, test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x, train_set_y, test_set_x, test_set_y = data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train_set_x shape: (784, 50000)\ntrain_set_y shape: (10, 50000)\ntest_set_x shape: (784, 10000)\ntest_set_y shape: (10, 10000)\n"
    }
   ],
   "source": [
    "print (\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data_wrapper has converted the training and validation data into numpy array of desired shapes. Let's convert the actual labels into a dataframe to see if the one hot conversions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The target dataset is:[5 0 4 ... 8 4 8]\nThe one hot encoding dataset is:\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   0      1      2      3      4      5      6      7      8      9      ...  \\\n0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n1    0.0    0.0    0.0    1.0    0.0    0.0    1.0    0.0    1.0    0.0  ...   \n2    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0  ...   \n3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0  ...   \n4    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0  ...   \n5    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n6    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n7    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n8    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n9    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0  ...   \n\n   49990  49991  49992  49993  49994  49995  49996  49997  49998  49999  \n0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0  \n1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n2    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0  \n3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n4    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0    0.0  \n5    0.0    1.0    1.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0  \n6    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n7    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n8    1.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    1.0  \n9    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n\n[10 rows x 50000 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>49990</th>\n      <th>49991</th>\n      <th>49992</th>\n      <th>49993</th>\n      <th>49994</th>\n      <th>49995</th>\n      <th>49996</th>\n      <th>49997</th>\n      <th>49998</th>\n      <th>49999</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows Ã— 50000 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "print(\"The target dataset is:\" + str(training_data[1]))\n",
    "print(\"The one hot encoding dataset is:\")\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualise the dataset. Feel free to change the index to see if the training data has been correctly tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7ffb5c470c90>"
     },
     "metadata": {},
     "execution_count": 14
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"263.63625pt\" version=\"1.1\" viewBox=\"0 0 251.565 263.63625\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 263.63625 \nL 251.565 263.63625 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 239.758125 \nL 244.365 239.758125 \nL 244.365 22.318125 \nL 26.925 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p77cacd8901)\">\n    <image height=\"218\" id=\"image1c9695f942\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-21.758125\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m015e983cfb\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m015e983cfb\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(27.626607 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m015e983cfb\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(66.455179 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m015e983cfb\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(102.1025 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m015e983cfb\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m015e983cfb\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(179.759643 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m015e983cfb\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m12b753b5b9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m12b753b5b9\" y=\"26.200982\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 30.000201)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m12b753b5b9\" y=\"65.029554\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 68.828772)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m12b753b5b9\" y=\"103.858125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 107.657344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m12b753b5b9\" y=\"142.686696\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 146.485915)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m12b753b5b9\" y=\"181.515268\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 185.314487)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m12b753b5b9\" y=\"220.343839\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 224.143058)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 239.758125 \nL 26.925 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 239.758125 \nL 244.365 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 239.758125 \nL 244.365 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 22.318125 \nL 244.365 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_13\">\n    <!-- Label is 3 -->\n    <defs>\n     <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n     <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n     <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n    </defs>\n    <g transform=\"translate(107.033437 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"55.712891\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"116.992188\" xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"180.46875\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"241.992188\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"269.775391\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"301.5625\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"329.345703\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"381.445312\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"413.232422\" xlink:href=\"#DejaVuSans-51\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p77cacd8901\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "index  = 1021\n",
    "k = train_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label= training_data[1][index]))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid\n",
    "This is one of the activation functions. It takes the cumulative input to the layer, the matrix **Z**, as the input. Upon application of the **`sigmoid`** function, the output matrix **H** is calculated. Also, **Z** is stored as the variable **sigmoid_memory** since it will be later used in backpropagation.You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ here in the following way. The exponential gets applied to all the elements of Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # sigmoid_memory is stored as it is used later on in backpropagation\n",
    "    \n",
    "    H = 1/(1+np.exp(-Z))\n",
    "    sigmoid_memory = Z\n",
    "    \n",
    "    return H, sigmoid_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sigmoid(Z) = (array([[0.5       , 0.73105858],\n       [0.88079708, 0.95257413],\n       [0.98201379, 0.99330715],\n       [0.99752738, 0.99908895]]), array([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7]]))\n"
    }
   ],
   "source": [
    "Z = np.arange(8).reshape(4,2)\n",
    "print (\"sigmoid(Z) = \" + str(sigmoid(Z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu\n",
    "This is one of the activation functions. It takes the cumulative input to the layer, matrix **Z** as the input. Upon application of the **`relu`** function, matrix **H** which is the output matrix is calculated. Also, **Z** is stored as **relu_memory** which will be later used in backpropagation. You use _[np.maximum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.maximum.html)_ here in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # relu_memory is stored as it is used later on in backpropagation\n",
    "    \n",
    "    H = np.maximum(0,Z)\n",
    "    \n",
    "    assert(H.shape == Z.shape)\n",
    "    \n",
    "    relu_memory = Z \n",
    "    return H, relu_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "relu(Z) = (array([[ 1,  3],\n       [ 0,  0],\n       [ 0,  7],\n       [ 9, 18]]), array([[ 1,  3],\n       [-1, -4],\n       [-5,  7],\n       [ 9, 18]]))\n"
    }
   ],
   "source": [
    "Z = np.array([1, 3, -1, -4, -5, 7, 9, 18]).reshape(4,2)\n",
    "print (\"relu(Z) = \" + str(relu(Z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax\n",
    "This is the activation of the last layer. It takes the cumulative input to the layer, matrix **Z** as the input. Upon application of the **`softmax`** function, the output matrix **H** is calculated. Also, **Z** is stored as **softmax_memory** which will be later used in backpropagation. You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ and _[np.sum()](https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.sum.html)_ here in the following way. The exponential gets applied to all the elements of Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # softmax_memory is stored as it is used later on in backpropagation\n",
    "   \n",
    "    Z_exp = np.exp(Z)\n",
    "\n",
    "    Z_sum = np.sum(Z_exp,axis = 0, keepdims = True)\n",
    "    \n",
    "    H = Z_exp/Z_sum  #normalising step\n",
    "    softmax_memory = Z\n",
    "    \n",
    "    return H, softmax_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.array([[11,19,10], [12, 21, 23]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[2.68941421e-01 1.19202922e-01 2.26032430e-06]\n [7.31058579e-01 8.80797078e-01 9.99997740e-01]]\n[[11 19 10]\n [12 21 23]]\n"
    }
   ],
   "source": [
    "#Z = np.array(np.arange(30)).reshape(10,3)\n",
    "H, softmax_memory = softmax(Z)\n",
    "print(H)\n",
    "print(softmax_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize_parameters\n",
    "Let's now create a function **`initialize_parameters`** which initializes the weights and biases of the various layers. One way to initialise is to set all the parameters to 0. This is not a considered a good strategy as all the neurons will behave the same way and it'll defeat the purpose of deep networks. Hence, we initialize the weights randomly to very small values but not zeros. The biases are initialized to 0. Note that the **`initialize_parameters`** function initializes the parameters for all the layers in one `for` loop. \n",
    "\n",
    "The inputs to this function is a list named `dimensions`. The length of the list is the number layers in the network + 1 (the plus one is for the input layer, rest are hidden + output). The first element of this list is the dimensionality or length of the input (784 for the MNIST dataset). The rest of the list contains the number of neurons in the corresponding (hidden and output) layers.\n",
    "\n",
    "For example `dimensions = [784, 3, 7, 10]` specifies a network for the MNIST dataset with two hidden layers and a 10-dimensional softmax output.\n",
    "\n",
    "Also, notice that the parameters are returned in a dictionary. This will help you in implementing the feedforward through the layer and the backprop throught the layer at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dimensions):\n",
    "\n",
    "    # dimensions is a list containing the number of neuron in each layer in the network\n",
    "    # It returns parameters which is a python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "\n",
    "    np.random.seed(2)\n",
    "    parameters = {}\n",
    "    L = len(dimensions)            # number of layers in the network + 1\n",
    "\n",
    "    for l in range(1, L): \n",
    "        parameters['W' + str(l)] = np.random.randn(dimensions[l], dimensions[l-1]) * 0.1\n",
    "        parameters['b' + str(l)] = np.zeros((dimensions[l], 1)) \n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (dimensions[l], dimensions[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (dimensions[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "W1 = [[-0.04167578 -0.00562668 -0.21361961 ... -0.06168445  0.03213358\n  -0.09464469]\n [-0.05301394 -0.1259207   0.16775441 ... -0.03284246 -0.05623108\n   0.01179136]\n [ 0.07386378 -0.15872956  0.01532001 ... -0.08428557  0.10040469\n   0.00545832]]\nb1 = [[0.]\n [0.]\n [0.]]\nW2 = [[ 0.06650944 -0.19626047  0.2112715 ]\n [-0.28074571 -0.13967752  0.02641189]\n [ 0.10925169  0.06646016  0.08565535]\n [-0.11058228  0.03715795  0.13440124]\n [-0.16421272 -0.1153127   0.02013163]\n [ 0.13985659  0.07228733 -0.10717236]\n [-0.05673344 -0.03663499 -0.15460347]]\nb2 = [[0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]]\n"
    }
   ],
   "source": [
    "dimensions  = [784, 3,7,10]\n",
    "parameters = initialize_parameters(dimensions)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "# print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "# print(\"b3 = \" + str(parameters[\"b3\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer_forward\n",
    "\n",
    "The function **`layer_forward`** implements the forward propagation for a certain layer 'l'. It calculates the cumulative input into the layer **Z** and uses it to calculate the output of the layer **H**. It takes **H_prev, W, b and the activation function** as inputs and stores the **linear_memory, activation_memory** in the variable **memory** which will be used later in backpropagation. \n",
    "\n",
    "<br> You have to first calculate the **Z**(using the forward propagation equation), **linear_memory**(H_prev, W, b) and then calculate **H, activation_memory**(Z) by applying activation functions - **`sigmoid`**, **`relu`** and **`softmax`** on **Z**.\n",
    "\n",
    "<br> Note that $$H^{L-1}$$ is referred here as H_prev. You might want to use _[np.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)_ to carry out the matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def layer_forward(H_prev, W, b, activation = 'relu'):\n",
    "\n",
    "    # H_prev is of shape (size of previous layer, number of examples)\n",
    "    # W is weights matrix of shape (size of current layer, size of previous layer)\n",
    "    # b is bias vector of shape (size of the current layer, 1)\n",
    "    # activation is the activation to be used for forward propagation : \"softmax\", \"relu\", \"sigmoid\"\n",
    "\n",
    "    # H is the output of the activation function \n",
    "    # memory is a python dictionary containing \"linear_memory\" and \"activation_memory\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z = W@H_prev+b\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = sigmoid(Z)\n",
    " \n",
    "    elif activation == \"softmax\":\n",
    "        Z = W@H_prev+b\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = softmax(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z = W@H_prev+b\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = relu(Z)\n",
    "        \n",
    "    assert (H.shape == (W.shape[0], H_prev.shape[1]))\n",
    "    memory = (linear_memory, activation_memory)\n",
    "\n",
    "    return H, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1.        , 1.        , 1.        , 1.        , 1.        ],\n       [0.99908895, 0.99330715, 0.99999969, 1.        , 0.99987661],\n       [0.73105858, 0.5       , 0.99330715, 0.9999546 , 0.88079708]])"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# verify\n",
    "# l-1 has two neurons, l has three, m = 5\n",
    "# H_prev is (l-1, m)\n",
    "# W is (l, l-1)\n",
    "# b is (l, 1)\n",
    "# H should be (l, m)\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    "\n",
    "H = layer_forward(H_prev, W_sample, b_sample, activation=\"sigmoid\")[0]\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "    array([[1.        , 1.        , 1.        , 1.        , 1.        ],<br>\n",
    "      [0.99908895, 0.99330715, 0.99999969, 1.        , 0.99987661],<br>\n",
    "       [0.73105858, 0.5       , 0.99330715, 0.9999546 , 0.88079708]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_layer_forward\n",
    "**`L_layer_forward`** performs one forward pass through the whole network for all the training samples (note that we are feeding all training examples in one single batch). Use the **`layer_forward`** you have created above here to perform the feedforward for layers 1 to 'L-1' in the for loop with the activation **`relu`**. The last layer having a different activation **`softmax`** is calculated outside the loop. Notice that the **memory** is appended to **memories** for all the layers. These will be used in the backward order during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def L_layer_forward(X, parameters):\n",
    "\n",
    "    # X is input data of shape (input size, number of examples)\n",
    "    # parameters is output of initialize_parameters()\n",
    "    \n",
    "    # HL is the last layer's post-activation value\n",
    "    # memories is the list of memory containing (for a relu activation, for example):\n",
    "    # - every memory of relu forward (there are L-1 of them, indexed from 1 to L-1), \n",
    "    # - the memory of softmax forward (there is one, indexed L) \n",
    "\n",
    "    memories = []\n",
    "    H = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement relu layer (L-1) times as the Lth layer is the softmax layer\n",
    "    for l in range(1, L):\n",
    "        H_prev = H \n",
    "        Weight = parameters['W'+str(l)]\n",
    "        bias = parameters['b'+str(l)]\n",
    "        H, memory = layer_forward(H_prev, Weight, bias, activation = 'relu')\n",
    "        \n",
    "        memories.append(memory)\n",
    "    \n",
    "    # Implement the final softmax layer\n",
    "    # HL here is the final prediction P as specified in the lectures\n",
    "    H_prev = H\n",
    "    Weight = parameters['W'+str(L)]\n",
    "    bias = parameters['b'+str(L)]\n",
    "    HL, memory = layer_forward(H, Weight, bias, activation = 'softmax')\n",
    "    \n",
    "    memories.append(memory)\n",
    "\n",
    "    assert(HL.shape == (10, X.shape[1]))\n",
    "            \n",
    "    return HL, memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(784, 10)\n[[0.10106734 0.10045152 0.09927757 0.10216656 0.1       ]\n [0.10567625 0.10230873 0.10170271 0.11250099 0.1       ]\n [0.09824287 0.0992886  0.09967128 0.09609693 0.1       ]\n [0.10028288 0.10013048 0.09998149 0.10046076 0.1       ]\n [0.09883601 0.09953443 0.09931419 0.097355   0.1       ]\n [0.10668575 0.10270912 0.10180736 0.11483609 0.1       ]\n [0.09832513 0.09932275 0.09954792 0.09627089 0.1       ]\n [0.09747092 0.09896735 0.0995387  0.09447277 0.1       ]\n [0.09489069 0.09788255 0.09929998 0.08915178 0.1       ]\n [0.09852217 0.09940447 0.09985881 0.09668824 0.1       ]]\n"
    }
   ],
   "source": [
    "# verify\n",
    "# X is (784, 10)\n",
    "# parameters is a dict\n",
    "# HL should be (10, 10)\n",
    "x_sample = train_set_x[:, 10:20]\n",
    "print(x_sample.shape)\n",
    "HL = L_layer_forward(x_sample, parameters=parameters)[0]\n",
    "print(HL[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:\n",
    "\n",
    "(784, 10)<br>\n",
    "[[0.10106734 0.10045152 0.09927757 0.10216656 0.1       ]<br>\n",
    " [0.10567625 0.10230873 0.10170271 0.11250099 0.1       ]<br>\n",
    " [0.09824287 0.0992886  0.09967128 0.09609693 0.1       ]<br>\n",
    " [0.10028288 0.10013048 0.09998149 0.10046076 0.1       ]<br>\n",
    " [0.09883601 0.09953443 0.09931419 0.097355   0.1       ]<br>\n",
    " [0.10668575 0.10270912 0.10180736 0.11483609 0.1       ]<br>\n",
    " [0.09832513 0.09932275 0.09954792 0.09627089 0.1       ]<br>\n",
    " [0.09747092 0.09896735 0.0995387  0.09447277 0.1       ]<br>\n",
    " [0.09489069 0.09788255 0.09929998 0.08915178 0.1       ]<br>\n",
    " [0.09852217 0.09940447 0.09985881 0.09668824 0.1       ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "### compute_loss\n",
    "The next step is to compute the loss function after every forward pass to keep checking whether it is decreasing with training.<br> **`compute_loss`** here calculates the cross-entropy loss. You may want to use _[np.log()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html)_, _[np.sum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html)_, _[np.multiply()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.multiply.html)_ here. Do not forget that it is the average loss across all the data points in the batch. It takes the output of the last layer **HL** and the ground truth label **Y** as input and returns the **loss**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def compute_loss(HL, Y):\n",
    "\n",
    "\n",
    "    # HL is probability matrix of shape (10, number of examples)\n",
    "    # Y is true \"label\" vector shape (10, number of examples)\n",
    "\n",
    "    # loss is the cross-entropy loss\n",
    "\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    loss = -(1./m) * np.sum(Y*np.log(HL)) \n",
    "    \n",
    "    loss = np.squeeze(loss)      # To make sure that the loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678 ]\n [0.33033482 0.20464863 0.61927097 0.29965467 0.26682728]\n [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987]\n [0.78533515 0.85397529 0.49423684 0.84656149 0.07964548]\n [0.50524609 0.0652865  0.42812233 0.09653092 0.12715997]\n [0.59674531 0.226012   0.10694568 0.22030621 0.34982629]\n [0.46778748 0.20174323 0.64040673 0.48306984 0.50523672]\n [0.38689265 0.79363745 0.58000418 0.1622986  0.70075235]\n [0.96455108 0.50000836 0.88952006 0.34161365 0.56714413]\n [0.42754596 0.43674726 0.77655918 0.53560417 0.95374223]]\n[[0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0.]\n [1. 0. 1. 0. 0.]\n [0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0.]]\n0.8964600261334037\n"
    }
   ],
   "source": [
    "# sample\n",
    "# HL is (10, 5), Y is (10, 5)\n",
    "np.random.seed(2)\n",
    "HL_sample = np.random.rand(10,5)\n",
    "Y_sample = train_set_y[:, 10:15]\n",
    "print(HL_sample)\n",
    "print(Y_sample)\n",
    "\n",
    "print(compute_loss(HL_sample, Y_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "    \n",
    "[[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678 ]<br>\n",
    " [0.33033482 0.20464863 0.61927097 0.29965467 0.26682728]<br>\n",
    " [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987]<br>\n",
    " [0.78533515 0.85397529 0.49423684 0.84656149 0.07964548]<br>\n",
    " [0.50524609 0.0652865  0.42812233 0.09653092 0.12715997]<br>\n",
    " [0.59674531 0.226012   0.10694568 0.22030621 0.34982629]<br>\n",
    " [0.46778748 0.20174323 0.64040673 0.48306984 0.50523672]<br>\n",
    " [0.38689265 0.79363745 0.58000418 0.1622986  0.70075235]<br>\n",
    " [0.96455108 0.50000836 0.88952006 0.34161365 0.56714413]<br>\n",
    " [0.42754596 0.43674726 0.77655918 0.53560417 0.95374223]]<br>\n",
    "[[0. 0. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 1.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [1. 0. 1. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [0. 1. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 1. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]<br>\n",
    " [0. 0. 0. 0. 0.]]<br>\n",
    "0.8964600261334037"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "Let's now get to the next step - backpropagation. Let's start with sigmoid_backward.\n",
    "\n",
    "### sigmoid-backward\n",
    "You might remember that we had created **`sigmoid`** function that calculated the activation for forward propagation. Now, we need the activation backward, which helps in calculating **dZ** from **dH**. Notice that it takes input **dH** and **sigmoid_memory** as input. **sigmoid_memory** is the **Z** which we had calculated during forward propagation. You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ here the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dH, sigmoid_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a sigmoid function\n",
    "    # dH is gradient of the sigmoid activated activation of shape same as H or Z in the same layer    \n",
    "    # sigmoid_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = sigmoid_memory\n",
    "    \n",
    "    H = 1/(1+np.exp(-Z))\n",
    "    dZ = dH * H * (1-H)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu-backward\n",
    "You might remember that we had created **`relu`** function that calculated the activation for forward propagation. Now, we need the activation backward, which helps in calculating **dZ** from **dH**. Notice that it takes input **dH** and **relu_memory** as input. **relu_memory** is the **Z** which we calculated uring forward propagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dH, relu_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a relu function\n",
    "    # dH is gradient of the relu activated activation of shape same as H or Z in the same layer    \n",
    "    # relu_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = relu_memory\n",
    "    dZ = np.array(dH, copy=True) # dZ will be the same as dA wherever the elements of A weren't 0\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer_backward\n",
    "\n",
    "**`layer_backward`** is a complimentary function of **`layer_forward`**. Like **`layer_forward`** calculates **H** using **W**, **H_prev** and **b**, **`layer_backward`** uses **dH** to calculate **dW**, **dH_prev** and **db**. You have already studied the formulae in backpropogation. To calculate **dZ**, use the **`sigmoid_backward`** and **`relu_backward`** function. You might need to use _[np.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)_, _[np.sum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html)_ for the rest. Remember to choose the axis correctly in db. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def layer_backward(dH, memory, activation = 'relu'):\n",
    "    \n",
    "    # takes dH and the memory calculated in layer_forward and activation as input to calculate the dH_prev, dW, db\n",
    "    # performs the backprop depending upon the activation function\n",
    "    \n",
    "\n",
    "    linear_memory, activation_memory = memory\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dH,activation_memory)\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW = (1./m)*(dZ@H_prev.T)\n",
    "        db = (1./m) * np.sum(dZ, axis=-1, keepdims=True)\n",
    "        dH_prev = W.T@dZ\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dH,activation_memory)\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW = (1./m)*(dZ@H_prev.T)\n",
    "        db = (1./m) * np.sum(dZ, axis=-1, keepdims=True)\n",
    "        dH_prev = W.T@dZ\n",
    "    \n",
    "    return dH_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dH_prev is \n [[5.6417525  0.66855959 6.86974666 5.46611139 4.92177244]\n [2.17997451 0.12963116 2.74831239 2.17661196 2.10183901]]\ndW is \n [[1.67565336 1.56891359]\n [1.39137819 1.4143854 ]\n [1.3597389  1.43013369]]\ndb is \n [[0.37345476]\n [0.34414727]\n [0.29074635]]\n"
    }
   ],
   "source": [
    "# verify\n",
    "# l-1 has two neurons, l has three, m = 5\n",
    "# H_prev is (l-1, m)\n",
    "# W is (l, l-1)\n",
    "# b is (l, 1)\n",
    "# H should be (l, m)\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    "\n",
    "H, memory = layer_forward(H_prev, W_sample, b_sample, activation=\"relu\")\n",
    "np.random.seed(2)\n",
    "dH = np.random.rand(3,5)\n",
    "dH_prev, dW, db = layer_backward(dH, memory, activation = 'relu')\n",
    "print('dH_prev is \\n' , dH_prev)\n",
    "print('dW is \\n' ,dW)\n",
    "print('db is \\n', db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "dH_prev is <br>\n",
    " [[5.6417525  0.66855959 6.86974666 5.46611139 4.92177244]<br>\n",
    " [2.17997451 0.12963116 2.74831239 2.17661196 2.10183901]]<br>\n",
    "dW is <br>\n",
    " [[1.67565336 1.56891359]<br>\n",
    " [1.39137819 1.4143854 ]<br>\n",
    " [1.3597389  1.43013369]]<br>\n",
    "db is <br>\n",
    " [[0.37345476]<br>\n",
    " [0.34414727]<br>\n",
    " [0.29074635]]<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L_layer_backward\n",
    "\n",
    "**`L_layer_backward`** performs backpropagation for the whole network. Recall that the backpropagation for the last layer, i.e. the softmax layer, is different from the rest, hence it is outside the reversed `for` loop. You need to use the function **`layer_backward`** here in the loop with the activation function as **`relu`**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def L_layer_backward(HL, Y, memories):\n",
    "    \n",
    "    # Takes the predicted value HL and the true target value Y and the \n",
    "    # memories calculated by L_layer_forward as input\n",
    "    \n",
    "    # returns the gradients calulated for all the layers as a dict\n",
    "\n",
    "    gradients = {}\n",
    "    L = len(memories) # the number of layers\n",
    "    m = HL.shape[1]\n",
    "    Y = Y.reshape(HL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Perform the backprop for the last layer that is the softmax layer\n",
    "    current_memory = memories[-1]\n",
    "    linear_memory, activation_memory = current_memory\n",
    "    dZ = HL - Y\n",
    "    H_prev, W, b = linear_memory\n",
    "    # Use the expressions you have used in 'layer_backward'\n",
    "    gradients[\"dH\" + str(L-1)] = W.T@dZ\n",
    "    gradients[\"dW\" + str(L)] = (1./m)*(dZ@H_prev.T)\n",
    "    gradients[\"db\" + str(L)] = (1./m)*np.sum(dZ, axis=-1, keepdims=True)\n",
    "    # Perform the backpropagation l-1 times\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Lth layer gradients: \"gradients[\"dH\" + str(l + 1)] \", gradients[\"dW\" + str(l + 2)] , gradients[\"db\" + str(l + 2)]\n",
    "        current_memory = memories[l]\n",
    "        out_gradient=gradients[\"dH\"+ str(l+1)]\n",
    "        dH_prev_temp, dW_temp, db_temp = layer_backward(out_gradient ,current_memory,activation=\"relu\")\n",
    "        gradients[\"dH\" + str(l)] = dH_prev_temp\n",
    "        gradients[\"dW\" + str(l + 1)] = dW_temp\n",
    "        gradients[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dW3 is \n [[ 0.02003701  0.0019043   0.01011729  0.0145757   0.00146444  0.00059863\n   0.        ]\n [ 0.02154547  0.00203519  0.01085648  0.01567075  0.00156469  0.00060533\n   0.        ]\n [-0.01718407 -0.00273711 -0.00499101 -0.00912135 -0.00207365  0.00059996\n   0.        ]\n [-0.01141498 -0.00158622 -0.00607049 -0.00924709 -0.00119619  0.00060381\n   0.        ]\n [ 0.01943173  0.0018421   0.00984543  0.01416368  0.00141676  0.00059682\n   0.        ]\n [ 0.01045447  0.00063974  0.00637621  0.00863306  0.00050118  0.00060441\n   0.        ]\n [-0.06338911 -0.00747251 -0.0242169  -0.03835708 -0.00581131  0.0006034\n   0.        ]\n [ 0.01911373  0.001805    0.00703101  0.0120636   0.00138836 -0.00140535\n   0.        ]\n [-0.01801603  0.0017357  -0.01489228 -0.02026076  0.00133528  0.00060264\n   0.        ]\n [ 0.0194218   0.00183381  0.00594427  0.01187949  0.00141043 -0.00340965\n   0.        ]]\ndb3 is \n [[ 0.10031756]\n [ 0.00460183]\n [-0.00142942]\n [-0.0997827 ]\n [ 0.09872663]\n [ 0.00536378]\n [-0.10124784]\n [-0.00191121]\n [-0.00359044]\n [-0.00104818]]\ndW2 is \n [[ 4.94428956e-05  1.13215514e-02  5.44180380e-02]\n [-4.81267081e-05 -2.96999448e-05 -1.81899582e-02]\n [ 5.63424333e-05  4.77190073e-03  4.04810232e-02]\n [ 1.49767478e-04 -1.89780927e-03 -7.91231369e-03]\n [ 1.97866094e-04  1.22107085e-04  2.64140566e-02]\n [ 0.00000000e+00 -3.75805770e-04  1.63906102e-05]\n [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]\ndb2 is \n [[ 0.013979  ]\n [-0.01329383]\n [ 0.01275707]\n [-0.01052957]\n [ 0.03179224]\n [-0.00039877]\n [ 0.        ]]\n"
    }
   ],
   "source": [
    "# verify\n",
    "# X is (784, 10)\n",
    "# parameters is a dict\n",
    "# HL should be (10, 10)\n",
    "x_sample = train_set_x[:, 10:20]\n",
    "y_sample = train_set_y[:, 10:20]\n",
    "\n",
    "HL, memories = L_layer_forward(x_sample, parameters=parameters)\n",
    "gradients  = L_layer_backward(HL, y_sample, memories)\n",
    "print('dW3 is \\n', gradients['dW3'])\n",
    "print('db3 is \\n', gradients['db3'])\n",
    "print('dW2 is \\n', gradients['dW2'])\n",
    "print('db2 is \\n', gradients['db2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:<br>\n",
    "\n",
    "dW3 is <br>\n",
    " [[ 0.02003701  0.0019043   0.01011729  0.0145757   0.00146444  0.00059863  0.        ]<br>\n",
    " [ 0.02154547  0.00203519  0.01085648  0.01567075  0.00156469  0.00060533   0.        ]<br>\n",
    " [-0.01718407 -0.00273711 -0.00499101 -0.00912135 -0.00207365  0.00059996   0.        ]<br>\n",
    " [-0.01141498 -0.00158622 -0.00607049 -0.00924709 -0.00119619  0.00060381   0.        ]<br>\n",
    " [ 0.01943173  0.0018421   0.00984543  0.01416368  0.00141676  0.00059682   0.        ]<br>\n",
    " [ 0.01045447  0.00063974  0.00637621  0.00863306  0.00050118  0.00060441   0.        ]<br>\n",
    " [-0.06338911 -0.00747251 -0.0242169  -0.03835708 -0.00581131  0.0006034   0.        ]<br>\n",
    " [ 0.01911373  0.001805    0.00703101  0.0120636   0.00138836 -0.00140535   0.        ]<br>\n",
    " [-0.01801603  0.0017357  -0.01489228 -0.02026076  0.00133528  0.00060264   0.        ]<br>\n",
    " [ 0.0194218   0.00183381  0.00594427  0.01187949  0.00141043 -0.00340965    0.        ]]<br>\n",
    "db3 is <br>\n",
    " [[ 0.10031756]<br>\n",
    " [ 0.00460183]<br>\n",
    " [-0.00142942]<br>\n",
    " [-0.0997827 ]<br>\n",
    " [ 0.09872663]<br>\n",
    " [ 0.00536378]<br>\n",
    " [-0.10124784]<br>\n",
    " [-0.00191121]<br>\n",
    " [-0.00359044]<br>\n",
    " [-0.00104818]]<br>\n",
    "dW2 is <br>\n",
    " [[ 4.94428956e-05  1.13215514e-02  5.44180380e-02]<br>\n",
    " [-4.81267081e-05 -2.96999448e-05 -1.81899582e-02]<br>\n",
    " [ 5.63424333e-05  4.77190073e-03  4.04810232e-02]<br>\n",
    " [ 1.49767478e-04 -1.89780927e-03 -7.91231369e-03]<br>\n",
    " [ 1.97866094e-04  1.22107085e-04  2.64140566e-02]<br>\n",
    " [ 0.00000000e+00 -3.75805770e-04  1.63906102e-05]<br>\n",
    " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]<br>\n",
    "db2 is <br>\n",
    " [[ 0.013979  ]<br>\n",
    " [-0.01329383]<br>\n",
    " [ 0.01275707]<br>\n",
    " [-0.01052957]<br>\n",
    " [ 0.03179224]<br>\n",
    " [-0.00039877]<br>\n",
    " [ 0.        ]]<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Updates\n",
    "\n",
    "Now that we have calculated the gradients. let's do the last step which is updating the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "\n",
    "    # parameters is the python dictionary containing the parameters W and b for all the layers\n",
    "    # gradients is the python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    # returns updated weights after applying the gradient descent update\n",
    "\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*gradients[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*gradients[\"db\" + str(l + 1)]\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the bits and pieces of the feedforward and the backpropagation, let's now combine all that to form a model. The list `dimensions` has the number of neurons in each layer specified in it. For a neural network with 1 hidden layer with 45 neurons, you would specify the dimensions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [784, 45, 10] #  three-layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "### L_layer_model\n",
    "\n",
    "This is a composite function which takes the training data as input **X**, ground truth label **Y**, the **dimensions** as stated above, **learning_rate**, the number of iterations **num_iterations** and if you want to print the loss, **print_loss**. You need to use the final functions we have written for feedforward, computing the loss, backpropagation and updating the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graded\n",
    "\n",
    "def L_layer_model(X, Y, dimensions, learning_rate = 0.0075, num_iterations = 3000, print_loss=False):\n",
    "    \n",
    "    # X and Y are the input training datasets\n",
    "    # learning_rate, num_iterations are gradient descent optimization parameters\n",
    "    # returns updated parameters\n",
    "\n",
    "    np.random.seed(2)\n",
    "    losses = []                         # keep track of loss\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initialize_parameters(dimensions)\n",
    " \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "        HL, memories = L_layer_forward(X, parameters)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(HL, Y)\n",
    "    \n",
    "        # Backward propagation\n",
    "        gradients = L_layer_backward(HL,Y, memories)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "                \n",
    "        # Printing the loss every 100 training example\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print (\"Loss after iteration %i: %f\" %(i, loss))\n",
    "            losses.append(loss)\n",
    "            \n",
    "    # plotting the loss\n",
    "    plt.plot(np.squeeze(losses))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, it'll take a lot of time to train the model on 50,000 data points, we take a subset of 5,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(784, 5000)"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "train_set_x_new = train_set_x[:,0:5000]\n",
    "train_set_y_new = train_set_y[:,0:5000]\n",
    "train_set_x_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's call the function L_layer_model on the dataset we have created.This will take 10-20 mins to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loss after iteration 0: 2.422624\nLoss after iteration 100: 2.129232\nLoss after iteration 200: 1.876095\nLoss after iteration 300: 1.604213\nLoss after iteration 400: 1.350205\nLoss after iteration 500: 1.144823\nLoss after iteration 600: 0.990554\nLoss after iteration 700: 0.876603\nLoss after iteration 800: 0.791154\nLoss after iteration 900: 0.725441\nLoss after iteration 1000: 0.673485\nLoss after iteration 1100: 0.631386\nLoss after iteration 1200: 0.596598\nLoss after iteration 1300: 0.567342\nLoss after iteration 1400: 0.542346\nLoss after iteration 1500: 0.520746\nLoss after iteration 1600: 0.501865\nLoss after iteration 1700: 0.485205\nLoss after iteration 1800: 0.470368\nLoss after iteration 1900: 0.457054\nLoss after iteration 2000: 0.445034\nLoss after iteration 2100: 0.434120\nLoss after iteration 2200: 0.424141\nLoss after iteration 2300: 0.414967\nLoss after iteration 2400: 0.406498\nLoss after iteration 2500: 0.398653\nLoss after iteration 2600: 0.391355\nLoss after iteration 2700: 0.384535\nLoss after iteration 2800: 0.378139\nLoss after iteration 2900: 0.372120\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 387.420866 277.314375\" width=\"387.420866pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 387.420866 277.314375 \nL 387.420866 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \nL 378.58125 22.318125 \nL 43.78125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m6b92e1b75f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#m6b92e1b75f\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(55.818182 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"111.475921\" xlink:href=\"#m6b92e1b75f\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(108.294671 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"163.95241\" xlink:href=\"#m6b92e1b75f\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(157.58991 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"216.428899\" xlink:href=\"#m6b92e1b75f\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(210.066399 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"268.905388\" xlink:href=\"#m6b92e1b75f\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(262.542888 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"321.381877\" xlink:href=\"#m6b92e1b75f\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(315.019377 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"373.858366\" xlink:href=\"#m6b92e1b75f\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(367.495866 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- iterations (per tens) -->\n     <defs>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 31 75.875 \nQ 24.46875 64.65625 21.28125 53.65625 \nQ 18.109375 42.671875 18.109375 31.390625 \nQ 18.109375 20.125 21.3125 9.0625 \nQ 24.515625 -2 31 -13.1875 \nL 23.1875 -13.1875 \nQ 15.875 -1.703125 12.234375 9.375 \nQ 8.59375 20.453125 8.59375 31.390625 \nQ 8.59375 42.28125 12.203125 53.3125 \nQ 15.828125 64.359375 23.1875 75.875 \nz\n\" id=\"DejaVuSans-40\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 8.015625 75.875 \nL 15.828125 75.875 \nQ 23.140625 64.359375 26.78125 53.3125 \nQ 30.421875 42.28125 30.421875 31.390625 \nQ 30.421875 20.453125 26.78125 9.375 \nQ 23.140625 -1.703125 15.828125 -13.1875 \nL 8.015625 -13.1875 \nQ 14.5 -2 17.703125 9.0625 \nQ 20.90625 20.125 20.90625 31.390625 \nQ 20.90625 42.671875 17.703125 53.65625 \nQ 14.5 64.65625 8.015625 75.875 \nz\n\" id=\"DejaVuSans-41\"/>\n     </defs>\n     <g transform=\"translate(161.257812 268.034687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"66.992188\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"128.515625\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"169.628906\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"230.908203\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"270.117188\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"297.900391\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"359.082031\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"422.460938\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"474.560547\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"506.347656\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"545.361328\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"608.837891\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"670.361328\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"711.474609\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"743.261719\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"782.470703\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"843.994141\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"907.373047\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"959.472656\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m4a151a52c8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4a151a52c8\" y=\"217.546621\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.5 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(20.878125 221.345839)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4a151a52c8\" y=\"169.345608\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.0 -->\n      <g transform=\"translate(20.878125 173.144826)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4a151a52c8\" y=\"121.144595\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.5 -->\n      <g transform=\"translate(20.878125 124.943813)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4a151a52c8\" y=\"72.943581\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 2.0 -->\n      <g transform=\"translate(20.878125 76.7428)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4a151a52c8\" y=\"24.742568\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 2.5 -->\n      <g transform=\"translate(20.878125 28.541787)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- loss -->\n     <defs>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     </defs>\n     <g transform=\"translate(14.798438 140.695937)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p86e9cd73a0)\" d=\"M 58.999432 32.201761 \nL 69.49473 60.485322 \nL 79.990027 84.888298 \nL 90.485325 111.098229 \nL 100.980623 135.585181 \nL 111.475921 155.384391 \nL 121.971219 170.256195 \nL 132.466516 181.241373 \nL 142.961814 189.478832 \nL 153.457112 195.813681 \nL 163.95241 200.822312 \nL 174.447708 204.880737 \nL 184.943005 208.234401 \nL 195.438303 211.054728 \nL 205.933601 213.46434 \nL 216.428899 215.546694 \nL 226.924197 217.366815 \nL 237.419495 218.972912 \nL 247.914792 220.403174 \nL 258.41009 221.686709 \nL 268.905388 222.845456 \nL 279.400686 223.897556 \nL 289.895984 224.859602 \nL 300.391281 225.744 \nL 310.886579 226.560438 \nL 321.381877 227.316638 \nL 331.877175 228.020211 \nL 342.372473 228.677724 \nL 352.86777 229.294305 \nL 363.363068 229.874489 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 239.758125 \nL 43.78125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 378.58125 239.758125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 22.318125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_15\">\n    <!-- Learning rate =0.0075 -->\n    <defs>\n     <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     <path d=\"M 10.59375 45.40625 \nL 73.1875 45.40625 \nL 73.1875 37.203125 \nL 10.59375 37.203125 \nz\nM 10.59375 25.484375 \nL 73.1875 25.484375 \nL 73.1875 17.1875 \nL 10.59375 17.1875 \nz\n\" id=\"DejaVuSans-61\"/>\n     <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n    </defs>\n    <g transform=\"translate(143.108438 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"53.962891\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"115.486328\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"176.765625\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"216.128906\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"279.507812\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"307.291016\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"370.669922\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"434.146484\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"465.933594\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"507.046875\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"568.326172\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"607.535156\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"669.058594\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"700.845703\" xlink:href=\"#DejaVuSans-61\"/>\n     <use x=\"784.634766\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"848.257812\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"880.044922\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"943.667969\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"1007.291016\" xlink:href=\"#DejaVuSans-55\"/>\n     <use x=\"1070.914062\" xlink:href=\"#DejaVuSans-53\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p86e9cd73a0\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "parameters = L_layer_model(train_set_x_new, train_set_y_new, dimensions, num_iterations = 3000, print_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \n",
    "    # Performs forward propogation using the trained parameters and calculates the accuracy\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_layer_forward(X, parameters)\n",
    "    \n",
    "    p = np.argmax(probas, axis = 0)\n",
    "    act = np.argmax(y, axis = 0)\n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == act)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the accuray we get on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.8982000000000002\n"
    }
   ],
   "source": [
    "pred_train = predict(train_set_x_new, train_set_y_new, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get ~ 88% accuracy on the training data. Let's see the accuray on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.8822000000000003\n"
    }
   ],
   "source": [
    "pred_test = predict(test_set_x, test_set_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is ~88%. You can train the model even longer and get better result. You can also try to change the network structure. \n",
    "<br>Below, you can see which all numbers are incorrectly identified by the neural network by changing the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7ffb56492050>"
     },
     "metadata": {},
     "execution_count": 44
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"263.63625pt\" version=\"1.1\" viewBox=\"0 0 251.565 263.63625\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 263.63625 \nL 251.565 263.63625 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 239.758125 \nL 244.365 239.758125 \nL 244.365 22.318125 \nL 26.925 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pf87d197b6b)\">\n    <image height=\"218\" id=\"image5c57c4938b\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-21.758125\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mbead471cab\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#mbead471cab\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(27.626607 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#mbead471cab\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(66.455179 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#mbead471cab\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(102.1025 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#mbead471cab\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#mbead471cab\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(179.759643 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#mbead471cab\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mcacdd6b5b5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcacdd6b5b5\" y=\"26.200982\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 30.000201)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcacdd6b5b5\" y=\"65.029554\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 68.828772)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcacdd6b5b5\" y=\"103.858125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 107.657344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcacdd6b5b5\" y=\"142.686696\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 146.485915)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcacdd6b5b5\" y=\"181.515268\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 185.314487)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mcacdd6b5b5\" y=\"220.343839\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 224.143058)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 239.758125 \nL 26.925 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 239.758125 \nL 244.365 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 239.758125 \nL 244.365 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 22.318125 \nL 244.365 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_13\">\n    <!-- Label is (4, 4) -->\n    <defs>\n     <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n     <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n     <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     <path d=\"M 31 75.875 \nQ 24.46875 64.65625 21.28125 53.65625 \nQ 18.109375 42.671875 18.109375 31.390625 \nQ 18.109375 20.125 21.3125 9.0625 \nQ 24.515625 -2 31 -13.1875 \nL 23.1875 -13.1875 \nQ 15.875 -1.703125 12.234375 9.375 \nQ 8.59375 20.453125 8.59375 31.390625 \nQ 8.59375 42.28125 12.203125 53.3125 \nQ 15.828125 64.359375 23.1875 75.875 \nz\n\" id=\"DejaVuSans-40\"/>\n     <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n     <path d=\"M 11.71875 12.40625 \nL 22.015625 12.40625 \nL 22.015625 4 \nL 14.015625 -11.625 \nL 7.71875 -11.625 \nL 11.71875 4 \nz\n\" id=\"DejaVuSans-44\"/>\n     <path d=\"M 8.015625 75.875 \nL 15.828125 75.875 \nQ 23.140625 64.359375 26.78125 53.3125 \nQ 30.421875 42.28125 30.421875 31.390625 \nQ 30.421875 20.453125 26.78125 9.375 \nQ 23.140625 -1.703125 15.828125 -13.1875 \nL 8.015625 -13.1875 \nQ 14.5 -2 17.703125 9.0625 \nQ 20.90625 20.125 20.90625 31.390625 \nQ 20.90625 42.671875 17.703125 53.65625 \nQ 14.5 64.65625 8.015625 75.875 \nz\n\" id=\"DejaVuSans-41\"/>\n    </defs>\n    <g transform=\"translate(94.720312 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"55.712891\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"116.992188\" xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"180.46875\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"241.992188\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"269.775391\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"301.5625\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"329.345703\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"381.445312\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"413.232422\" xlink:href=\"#DejaVuSans-40\"/>\n     <use x=\"452.246094\" xlink:href=\"#DejaVuSans-52\"/>\n     <use x=\"515.869141\" xlink:href=\"#DejaVuSans-44\"/>\n     <use x=\"547.65625\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"579.443359\" xlink:href=\"#DejaVuSans-52\"/>\n     <use x=\"643.066406\" xlink:href=\"#DejaVuSans-41\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pf87d197b6b\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAARVklEQVR4nO3dfZBV9X3H8fdHCiQaKw+W7Q5gsGqrTqvGYawjTGIGY5U6VceHiTMWVCqpjZNo0w5Kx4qtUSYTJXbaWNf6gIZqnaCVQTRRbNTaSQQVECXEh4EByvIg4kJFQfz2j3s2ueK95+7e593f5zWzs/ee73n4cvWz59xz7j0/RQRmNvgd1OoGzKw5HHazRDjsZolw2M0S4bCbJcJhN0uEwz5ASfqZpL+o97KSZkv6tyrXe6uka6pZtp/buU3SVY3ezmDjsLeYpHWSzmh1H70i4paI6PcfEUm/A0wD7ipR+3tJUc2/U9K0bNninr4PzJY0rL/rS5nDbvVyGbAkIvYUT5R0FHARsLm/K5Q0EpgNvF48PSI2A78E/qzaZlPksLcpSSMlLZa0TdJ72eNxB8x2lKSXJPVIelzSqKLlT5X0P5J2Slop6fQ+bneOpB9ljz8n6UeS3s3Ws0xSR5lFzwaeKzH9X4BZwN6+bP8AtwL/BGwvUfsZ8KdVrDNZDnv7Ogi4D/gicASwB/jnA+aZBlwBdAIfUwgGksYCTwA3A6OAvwEWZofa/TEdOAwYD4wG/jLro5Q/AtYWT5B0EfBRRCzp53aRdAowEfjXMrOsAU7s73pT5rC3qYh4NyIWRsQHEbEL+C7wlQNmezAiVkfE/wE3ABdLGgJcSuGQeklEfBIRTwPLgan9bGMfhZAfHRH7I+LliOgpM+8IYFfvE0mHArcA3+7nNsn+DT8Ero6IT8rMtivbpvWRw96mJB0s6S5J6yX1AM8DI7Ig9NpQ9Hg9MBQ4nMLRwEXZofdOSTuByRSOAPrjQeAnwMOS/lfS9yQNLTPve8ChRc/nUPhjtK6f2wT4K2BVRPw8Z55DgZ1VrDtZDnv7+g7wB8AfR8RvA1/OpqtonvFFj4+gsCfeTuGPwIMRMaLo55CImNufBiJiX0TcFBHHA6cB51B461DKKuD3i55PAb4lqVtSd9brI5Jm9WHTU4Dzi5Y9DbhNUvHbmOOAlf3596Tut1rdgAEwVNLnip5/TGHPtQfYmZ14u7HEcpdKegBYB/wD8OOI2J+dYFsm6U+AZyjs8U8F3oqIjX1tStJXKfzxeAPoofDHpNxh9RIKbzMWZM+nZNvttQz4a+DJbN1zgNMj4vQS67oMKH49HgV+DNxTNO0rQFWfB0iV9+ztYQmFYPf+zAF+AHyeQth+DjxVYrkHgfuBbgrh+BZARGwAzqVw2WobhT3939L//96/SyFkPRROiD2XbbOUB4Cpkj6f9fBuRHT3/gD7gfciYnc2/3jgxVIrioidByy7F+iJiPcBJHUCxwP/2c9/T9Lkm1dYvUi6BdgaET/ow7wrgCkR8W4V27kNeDsiflhFm8ly2M0S4cN4s0Q47GaJcNjNEtHUS2+SfILArMEiQqWm17Rnl3SWpLWS3pJ0XS3rMrPGqvpsfPaxzV8BXwM2UvjQxCUR8UbOMt6zmzVYI/bsp1D4RNY7EbEXeJjCBznMrA3VEvaxfPqLGBuzaZ8iaaak5ZKW17AtM6tRw0/QRUQX0AU+jDdrpVr27Jv49LeuxmXTzKwN1RL2ZcAxko7Mbvz3dWBRfdoys3qr+jA+Ij6WdDWFmxsMAe6NiNcrLGZmLdLUL8L4PbtZ4zXkQzVmNnA47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLRNVDNlv7GD16dNlaT09P7rL79u2rdztNc/TRR+fWX3jhhbK1s88+O3fZFStWVNVTO6sp7JLWAbuA/cDHETGxHk2ZWf3VY8/+1YjYXof1mFkD+T27WSJqDXsAP5X0sqSZpWaQNFPScknLa9yWmdWg1sP4yRGxSdIY4GlJv4yI54tniIguoAtAUtS4PTOrUk179ojYlP3eCjwGnFKPpsys/qoOu6RDJB3a+xg4E1hdr8bMrL5qOYzvAB6T1Luef4+Ip+rSlX3KyJEjc+uvvvpq2drNN9+cu2xXV1dVPbWDa6+9Nrc+ZsyYsrUjjjgid1lfZy8SEe8AJ9axFzNrIF96M0uEw26WCIfdLBEOu1kiHHazRPgrrgPAlVdemVsfO3Zs2dpLL71U73baxlVXXZVbf//998vWBvPrUo737GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZInydvQ2ceeaZufWbbroptz5r1qyytVWrVlXVUzuYN29eTcvfcccdZWvd3d01rXsg8p7dLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEIpo3SEuqI8Kcc845ufW77747t37QQfl/kzs6Ovrd00CQN+QywKRJk3Lrxx13XNna2rVrq+ppIIgIlZruPbtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgh/n70OTjwxfzDbStfRDz744Nz6BRdc0O+eBoJjjz02t17pOno2XLj1UcU9u6R7JW2VtLpo2ihJT0t6M/udP4C4mbVcXw7j7wfOOmDadcDSiDgGWJo9N7M2VjHsEfE8sOOAyecC87PH84Hz6tyXmdVZte/ZOyJic/a4Gyj74WxJM4GZVW7HzOqk5hN0ERF5X3CJiC6gC9L9IoxZO6j20tsWSZ0A2e+t9WvJzBqh2rAvAqZnj6cDj9enHTNrlIqH8ZIeAk4HDpe0EbgRmAs8ImkGsB64uJFNtoPhw4eXrc2dOzd32TFjxuTWn3322dz6M888k1sfqPJeU4BK91rYuHFjbn3HjgPPK6etYtgj4pIypSl17sXMGsgflzVLhMNulgiH3SwRDrtZIhx2s0T4K659dNZZB34X6DcqDbm8fv363PqMGTOq6mmgyxtqui8q3Wp627ZtNa1/sPGe3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhK+z99Hu3bvL1vbv35+7bE9PT031weqMM86oaflUP59QLe/ZzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEqNLteuu6sUE6Isx9992XW582bVpu/e23386tz5s3L7d+55135tZbafLkyWVrlW6RPXTo0Nz6kCFDquppsIuIkmNZe89ulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXC19nrYMSIEbn122+/Pbd++eWX59Zr+W+0YMGC3PpHH31U9boBFi5cmFs/4YQTytZuvfXW3GUXL16cW7/00ktz6/v27Stb27NnT+6yA1nV19kl3Stpq6TVRdPmSNokaUX2M7WezZpZ/fXlMP5+oNRwKPMi4qTsZ0l92zKzeqsY9oh4HtjRhF7MrIFqOUF3taRV2WH+yHIzSZopabmk5TVsy8xqVG3Y7wSOAk4CNgO3lZsxIroiYmJETKxyW2ZWB1WFPSK2RMT+iPgEuBs4pb5tmVm9VRV2SZ1FT88HVpeb18zaQ8Xr7JIeAk4HDge2ADdmz08CAlgHfCMiNlfc2CC9zl7JYYcdllu/8MILc+vXX399bn306NFla5XuaS+VvCT7ayNHlj0dA9T2GYBabdiwIbe+ffv2srWJEwfvu8py19krDhIREZeUmHxPzR2ZWVP547JmiXDYzRLhsJslwmE3S4TDbpYIf8V1ABg2bFhufdSoUWVreUNN92XdebeCBjj55JNz6zfccEPZ2q5du3KXveuuu3LrL774Ym597969ZWtPPfVU7rIDmW8lbZY4h90sEQ67WSIcdrNEOOxmiXDYzRLhsJslouK33qz18q4XA3R3dzds24sWLcqtH3nkkVWve9OmTbn1WbNmVb1u+yzv2c0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRPg6u9Vk+PDhVS/7xBNP1LETq8R7drNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEX0Zsnk88ADQQWGI5q6IuEPSKOA/gAkUhm2+OCLeq7Au3zd+kFmzZk1u/YMPPihbmzRpUu6yH374YVU9pa6W+8Z/DHwnIo4HTgW+Kel44DpgaUQcAyzNnptZm6oY9ojYHBGvZI93AWuAscC5wPxstvnAeY1q0sxq16/37JImAF8CfgF0RMTmrNRN4TDfzNpUnz8bL+kLwELgmojokX7ztiAiotz7cUkzgZm1NmpmtenTnl3SUApBXxARj2aTt0jqzOqdwNZSy0ZEV0RMjIiJ9WjYzKpTMewq7MLvAdZExO1FpUXA9OzxdODx+rdnZvXSl8P4ScCfA69JWpFNmw3MBR6RNANYD1zcmBatlU477bTc+oQJE3LrTz75ZNmaL601V8WwR8R/AyWv2wFT6tuOmTWKP0FnlgiH3SwRDrtZIhx2s0Q47GaJcNjNEuFbSVuuK664Irc+bNiw3PrSpUvr2Y7VwHt2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRFW8lXdeN+VbSA87KlStz652dnbn1cePGla3t3bu3qp4sXy23kjazQcBhN0uEw26WCIfdLBEOu1kiHHazRDjsZonw99kt1/Dhw3Prixcvzq37Wnr78J7dLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0tExe+zSxoPPAB0AAF0RcQdkuYAVwLbsllnR8SSCuvy99nNGqzc99n7EvZOoDMiXpF0KPAycB5wMbA7Ir7f1yYcdrPGKxf2ip+gi4jNwObs8S5Ja4Cx9W3PzBqtX+/ZJU0AvgT8Ipt0taRVku6VNLLMMjMlLZe0vKZOzawmfb4HnaQvAM8B342IRyV1ANspvI//RwqH+rkDg/kw3qzxqn7PDiBpKLAY+ElE3F6iPgFYHBF/WGE9DrtZg1V9w0lJAu4B1hQHPTtx1+t8YHWtTZpZ4/TlbPxk4AXgNeCTbPJs4BLgJAqH8euAb2Qn8/LW5T27WYPVdBhfLw67WeP5vvFmiXPYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEc0esnk7sL7o+eHZtHbUrr21a1/g3qpVz96+WK7Q1O+zf2bj0vKImNiyBnK0a2/t2he4t2o1qzcfxpslwmE3S0Srw97V4u3nadfe2rUvcG/VakpvLX3PbmbN0+o9u5k1icNuloiWhF3SWZLWSnpL0nWt6KEcSeskvSZpRavHp8vG0NsqaXXRtFGSnpb0Zva75Bh7LeptjqRN2Wu3QtLUFvU2XtJ/SXpD0uuSvp1Nb+lrl9NXU163pr9nlzQE+BXwNWAjsAy4JCLeaGojZUhaB0yMiJZ/AEPSl4HdwAO9Q2tJ+h6wIyLmZn8oR0bErDbpbQ79HMa7Qb2VG2b8Mlr42tVz+PNqtGLPfgrwVkS8ExF7gYeBc1vQR9uLiOeBHQdMPheYnz2eT+F/lqYr01tbiIjNEfFK9ngX0DvMeEtfu5y+mqIVYR8LbCh6vpH2Gu89gJ9KelnSzFY3U0JH0TBb3UBHK5spoeIw3s10wDDjbfPaVTP8ea18gu6zJkfEycDZwDezw9W2FIX3YO107fRO4CgKYwBuBm5rZTPZMOMLgWsioqe41srXrkRfTXndWhH2TcD4oufjsmltISI2Zb+3Ao9ReNvRTrb0jqCb/d7a4n5+LSK2RMT+iPgEuJsWvnbZMOMLgQUR8Wg2ueWvXam+mvW6tSLsy4BjJB0paRjwdWBRC/r4DEmHZCdOkHQIcCbtNxT1ImB69ng68HgLe/mUdhnGu9ww47T4tWv58OcR0fQfYCqFM/JvA3/Xih7K9PV7wMrs5/VW9wY8ROGwbh+FcxszgNHAUuBN4BlgVBv19iCFob1XUQhWZ4t6m0zhEH0VsCL7mdrq1y6nr6a8bv64rFkifILOLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0vE/wPLXpoglejBrQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "index  = 1222\n",
    "k = test_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label=(pred_test[index], np.argmax(test_set_y, axis = 0)[index])))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37464bitanaconda3virtualenv8652760fcd634987abf6d7724a6d257d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}